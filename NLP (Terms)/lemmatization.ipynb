{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP4zis1TCW74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **lemmatization**\n",
        "In Natural Language Processing (NLP), stemming and lemmatization are two commonly used techniques for reducing words to their base form. While both techniques aim to reduce the inflectional and derivational forms of words, they differ in their approach and the results they produce.\n",
        "\n",
        "Stemming is a process of reducing words to their base or root form by removing the suffixes from words. For example, the stemming algorithm might convert the words \"running\", \"runners\", and \"run\" to the base word \"run\". This technique is useful in cases where the context of the words is not important, such as in search engines, where stemming can help to retrieve documents that contain the root form of a word.\n",
        "\n",
        "Lemmatization, on the other hand, is a more sophisticated approach that involves identifying the morphological root of a word based on its context in a sentence. This technique takes into account the part of speech of a word and applies different normalization rules to different parts of speech. For example, the lemma of the word \"am\" is \"be\", while the lemma of the word \"running\" is \"run\".\n",
        "\n",
        "In Python, there are several libraries that implement these techniques. The most popular libraries for stemming are Porter Stemmer and Snowball Stemmer, while the most popular library for lemmatization is WordNet Lemmatizer. Here's an example of how to use the Porter Stemmer and WordNet Lemmatizer in Python:"
      ],
      "metadata": {
        "id": "3Io4_WWGCZKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet') # Download the 'wordnet' dataset\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown foxes jumped over the lazy dogs\"\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "print(stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ExhHBYCduq",
        "outputId": "38290c43-9335-433e-e4eb-4489505bc23b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    }
  ]
}